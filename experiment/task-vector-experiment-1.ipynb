{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2052874,"sourceType":"datasetVersion","datasetId":981292}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git --quiet\n!pip install timm ftfy regex tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:40:01.769122Z","iopub.execute_input":"2025-11-26T07:40:01.769354Z","iopub.status.idle":"2025-11-26T07:41:19.159993Z","shell.execute_reply.started":"2025-11-26T07:40:01.769325Z","shell.execute_reply":"2025-11-26T07:41:19.159038Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.19)\nRequirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.36.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.2.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->timm) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Download task vectors","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport sys\n\nprint(\"Downloading task_vectors source code...\")\n\n# Download just the source file we need\n!wget -q https://raw.githubusercontent.com/mlfoundations/task_vectors/main/src/task_vectors.py -O task_vectors.py\n\nprint(\"‚úì Downloaded task_vectors.py\")\n\n# Verify the file exists\nimport os\nif os.path.exists('task_vectors.py'):\n    print(\"‚úì task_vectors.py is ready to use\")\nelse:\n    print(\"‚úó Download failed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:41:19.161824Z","iopub.execute_input":"2025-11-26T07:41:19.162093Z","iopub.status.idle":"2025-11-26T07:41:22.152135Z","shell.execute_reply.started":"2025-11-26T07:41:19.162069Z","shell.execute_reply":"2025-11-26T07:41:22.151360Z"}},"outputs":[{"name":"stdout","text":"Downloading task_vectors source code...\n‚úì Downloaded task_vectors.py\n‚úì task_vectors.py is ready to use\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# CELL 2: Import and test\n# ============================================================================\n\n# Now you can import directly\nfrom task_vectors import TaskVector\n\nprint(\"‚úì TaskVector imported successfully!\")\n\n# Test it\nprint(\"\\nUsage:\")\nprint(\"  task_vector = TaskVector('pretrained.pt', 'finetuned.pt')\")\nprint(\"  enhancement = task_vector_balanced - task_vector_imbalanced\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:41:22.153088Z","iopub.execute_input":"2025-11-26T07:41:22.153290Z","iopub.status.idle":"2025-11-26T07:41:25.252269Z","shell.execute_reply.started":"2025-11-26T07:41:22.153268Z","shell.execute_reply":"2025-11-26T07:41:25.251485Z"}},"outputs":[{"name":"stdout","text":"‚úì TaskVector imported successfully!\n\nUsage:\n  task_vector = TaskVector('pretrained.pt', 'finetuned.pt')\n  enhancement = task_vector_balanced - task_vector_imbalanced\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"#¬†imports","metadata":{}},{"cell_type":"code","source":"import gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:41:25.253166Z","iopub.execute_input":"2025-11-26T07:41:25.253541Z","iopub.status.idle":"2025-11-26T07:41:25.257136Z","shell.execute_reply.started":"2025-11-26T07:41:25.253514Z","shell.execute_reply":"2025-11-26T07:41:25.256506Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms\nimport clip \nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport os\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\n# Import TaskVector class\nfrom task_vectors import TaskVector\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:41:25.257764Z","iopub.execute_input":"2025-11-26T07:41:25.258039Z","iopub.status.idle":"2025-11-26T07:41:29.943821Z","shell.execute_reply.started":"2025-11-26T07:41:25.258021Z","shell.execute_reply":"2025-11-26T07:41:29.943195Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nCUDA available: True\nUsing device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# DMID dataset","metadata":{}},{"cell_type":"code","source":"CONFIG = {\n    'clip_model': 'ViT-B/32',  # ‚Üê CHANGED: CLIP model instead of timm model\n    'img_size': 224,\n    'batch_size': 4,\n    'epochs': 3,\n    'lr': 1e-6,  # ‚Üê CHANGED: Lower LR for CLIP fine-tuning\n    'alpha': 0.3,\n    'seed': 42,\n    'max_train_samples': 120\n}\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.manual_seed(CONFIG['seed'])\nnp.random.seed(CONFIG['seed'])\ndef empty_cache():\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    gc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:41:29.944553Z","iopub.execute_input":"2025-11-26T07:41:29.944941Z","iopub.status.idle":"2025-11-26T07:41:29.955513Z","shell.execute_reply.started":"2025-11-26T07:41:29.944921Z","shell.execute_reply":"2025-11-26T07:41:29.954794Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"base_path = '/kaggle/input/miniddsm2'\n\nprint(\"=\"*70)\nprint(\"LOADING MINI-DDSM DATASET\")\nprint(\"=\"*70)\n\n# USE JPEG-8 version (has Normal class)\ndataset_version = 'MINI-DDSM-Complete-JPEG-8'\ndata_dir = Path(base_path) / dataset_version\n\nprint(f\"‚úì Using: {dataset_version}\")\nprint(f\"  Path: {data_dir}\")\n\n# Load metadata\nmetadata_file = data_dir / 'DataWMask.xlsx'\n\nif metadata_file.exists():\n    print(f\"‚úì Found metadata: {metadata_file.name}\")\n    df_meta = pd.read_excel(metadata_file)\n    print(f\"‚úì Loaded {len(df_meta)} records\")\n    print(f\"\\nColumns: {list(df_meta.columns)}\")\nelse:\n    print(\"‚ö† No metadata file found, will use folder structure only\")\n    df_meta = None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:41:29.957784Z","iopub.execute_input":"2025-11-26T07:41:29.958016Z","iopub.status.idle":"2025-11-26T07:41:31.355257Z","shell.execute_reply.started":"2025-11-26T07:41:29.957993Z","shell.execute_reply":"2025-11-26T07:41:31.354671Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nLOADING MINI-DDSM DATASET\n======================================================================\n‚úì Using: MINI-DDSM-Complete-JPEG-8\n  Path: /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPEG-8\n‚úì Found metadata: DataWMask.xlsx\n‚úì Loaded 7808 records\n\nColumns: ['fullPath', 'fileName', 'View', 'Side', 'Status', 'Tumour_Contour', 'Tumour_Contour2', 'Age', 'Density']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(df_meta.head(3))\n\n# Check Density (BI-RADS) distribution\nprint(f\"\\nBI-RADS (Density) Distribution:\")\nprint(df_meta['Density'].value_counts().sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:41:31.355997Z","iopub.execute_input":"2025-11-26T07:41:31.356308Z","iopub.status.idle":"2025-11-26T07:41:31.381614Z","shell.execute_reply.started":"2025-11-26T07:41:31.356289Z","shell.execute_reply":"2025-11-26T07:41:31.380883Z"}},"outputs":[{"name":"stdout","text":"                            fullPath               fileName View   Side  \\\n0   Benign\\0029\\C_0029_1.LEFT_CC.jpg   C_0029_1.LEFT_CC.jpg   CC   LEFT   \n1  Benign\\0029\\C_0029_1.LEFT_MLO.jpg  C_0029_1.LEFT_MLO.jpg  MLO   LEFT   \n2  Benign\\0029\\C_0029_1.RIGHT_CC.jpg  C_0029_1.RIGHT_CC.jpg   CC  RIGHT   \n\n   Status                          Tumour_Contour Tumour_Contour2   Age  \\\n0  Benign   Benign\\0029\\C_0029_1.LEFT_CC_Mask.jpg               -  66.0   \n1  Benign  Benign\\0029\\C_0029_1.LEFT_MLO_Mask.jpg               -  66.0   \n2  Benign                                       -               -  66.0   \n\n   Density  \n0        3  \n1        3  \n2        3  \n\nBI-RADS (Density) Distribution:\nDensity\n0       4\n1    1076\n2    3020\n3    2320\n4    1388\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*70)\nprint(\"MAPPING IMAGES TO BI-RADS CATEGORIES\")\nprint(\"=\"*70)\n\ndata = []\n\n# Iterate through class folders\nclass_folders = {\n    'Normal': data_dir / 'Normal',\n    'Benign': data_dir / 'Benign',\n    'Cancer': data_dir / 'Cancer'\n}\n\nfor class_name, class_path in class_folders.items():\n    if not class_path.exists():\n        continue\n    \n    print(f\"\\nProcessing {class_name}...\")\n    \n    # Find all case folders\n    case_folders = [f for f in class_path.iterdir() if f.is_dir()]\n    \n    for case_folder in case_folders:\n        case_id = case_folder.name\n        \n        # Find images (skip masks)\n        images = [img for img in case_folder.glob('*.jpg') \n                  if 'mask' not in img.name.lower() or 'boundary' in img.name.lower()]\n        \n        for img_path in images:\n            # Match with metadata using fileName or fullPath\n            matching_rows = df_meta[df_meta['fileName'].str.contains(case_id, na=False)]\n            \n            if len(matching_rows) == 0:\n                # Try fullPath match\n                matching_rows = df_meta[df_meta['fullPath'].str.contains(case_id, na=False)]\n            \n            if len(matching_rows) > 0:\n                row = matching_rows.iloc[0]\n                birads = row['Density']\n                age = row['Age']\n                status = row['Status']\n            else:\n                # Fallback: infer from class\n                birads = {'Normal': 1, 'Benign': 2, 'Cancer': 4}.get(class_name, 0)\n                age = 50\n                status = class_name\n            \n            data.append({\n                'image_id': case_id,\n                'image_path': str(img_path),\n                'classification': class_name,\n                'birads': birads,\n                'age': age,\n                'status': status\n            })\n\ndf = pd.DataFrame(data)\nprint(f\"\\n‚úì Mapped {len(df)} images\")\nprint(f\"\\nBI-RADS Distribution:\")\nprint(df['birads'].value_counts().sort_index())\nprint(f\"\\nClassification Distribution:\")\nprint(df['classification'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:41:31.382398Z","iopub.execute_input":"2025-11-26T07:41:31.382591Z","iopub.status.idle":"2025-11-26T07:42:11.952518Z","shell.execute_reply.started":"2025-11-26T07:41:31.382578Z","shell.execute_reply":"2025-11-26T07:42:11.951669Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nMAPPING IMAGES TO BI-RADS CATEGORIES\n======================================================================\n\nProcessing Normal...\n\nProcessing Benign...\n\nProcessing Cancer...\n\n‚úì Mapped 7808 images\n\nBI-RADS Distribution:\nbirads\n0       4\n1    1168\n2    3184\n3    2260\n4    1192\nName: count, dtype: int64\n\nClassification Distribution:\nclassification\nCancer    2716\nBenign    2684\nNormal    2408\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Create Artificial Imbalance","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimbalance_ratio = 10  # ratio of majority to minority\nmax_samples_per_class = 300  # cap for majority classes to quickly prototype\nnp.random.seed(CONFIG['seed'])\n\n# Downsample all classes\ndf_birads1 = df[df['birads'] == 1].sample(n=min(len(df[df['birads'] == 1]), max_samples_per_class), random_state=CONFIG['seed'])\ndf_birads2 = df[df['birads'] == 2].sample(n=min(len(df[df['birads'] == 2]), max_samples_per_class), random_state=CONFIG['seed'])\ndf_birads3 = df[df['birads'] == 3].sample(n=min(len(df[df['birads'] == 3]), max_samples_per_class), random_state=CONFIG['seed'])\n\n# Make BI-RADS 4 scarce\nbirads4_target = max_samples_per_class // imbalance_ratio  # ~30 samples\ndf_birads4 = df[df['birads'] == 4].sample(n=min(len(df[df['birads'] == 4]), birads4_target), random_state=CONFIG['seed'])\n\n# Combine and shuffle\ndf_downsampled = pd.concat([df_birads1, df_birads2, df_birads3, df_birads4]).sample(frac=1, random_state=CONFIG['seed']).reset_index(drop=True)\ndf_downsampled['birads_label'] = df_downsampled['birads'].astype(int) - 1\n\nprint('Downsampled counts:', df_downsampled['birads'].value_counts().sort_index())\nprint('Total samples:', len(df_downsampled))\ndf_downsampled\ndf = df_downsampled.copy()\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:42:11.953495Z","iopub.execute_input":"2025-11-26T07:42:11.953857Z","iopub.status.idle":"2025-11-26T07:42:11.986950Z","shell.execute_reply.started":"2025-11-26T07:42:11.953817Z","shell.execute_reply":"2025-11-26T07:42:11.986353Z"}},"outputs":[{"name":"stdout","text":"Downsampled counts: birads\n1    300\n2    300\n3    300\n4     30\nName: count, dtype: int64\nTotal samples: 930\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"    image_id                                         image_path  \\\n0       4167  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n1       0144  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n2       1850  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n3       0003  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n4       2010  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n..       ...                                                ...   \n925     0103  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n926     0031  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n927     0598  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n928     0271  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n929     1911  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n\n    classification  birads   age  status  birads_label  \n0           Benign       3  44.0  Benign             2  \n1           Cancer       3  74.0  Cancer             2  \n2           Cancer       1  82.0  Cancer             0  \n3           Normal       2  46.0  Cancer             1  \n4           Normal       1  74.0  Normal             0  \n..             ...     ...   ...     ...           ...  \n925         Cancer       1  50.0  Cancer             0  \n926         Normal       1  81.0  Cancer             0  \n927         Normal       3  52.0  Normal             2  \n928         Normal       2  53.0  Benign             1  \n929         Benign       1  67.0  Benign             0  \n\n[930 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>image_path</th>\n      <th>classification</th>\n      <th>birads</th>\n      <th>age</th>\n      <th>status</th>\n      <th>birads_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4167</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Benign</td>\n      <td>3</td>\n      <td>44.0</td>\n      <td>Benign</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0144</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Cancer</td>\n      <td>3</td>\n      <td>74.0</td>\n      <td>Cancer</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1850</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Cancer</td>\n      <td>1</td>\n      <td>82.0</td>\n      <td>Cancer</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0003</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Normal</td>\n      <td>2</td>\n      <td>46.0</td>\n      <td>Cancer</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Normal</td>\n      <td>1</td>\n      <td>74.0</td>\n      <td>Normal</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>925</th>\n      <td>0103</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Cancer</td>\n      <td>1</td>\n      <td>50.0</td>\n      <td>Cancer</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>926</th>\n      <td>0031</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Normal</td>\n      <td>1</td>\n      <td>81.0</td>\n      <td>Cancer</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>927</th>\n      <td>0598</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Normal</td>\n      <td>3</td>\n      <td>52.0</td>\n      <td>Normal</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>928</th>\n      <td>0271</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Normal</td>\n      <td>2</td>\n      <td>53.0</td>\n      <td>Benign</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>929</th>\n      <td>1911</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Benign</td>\n      <td>1</td>\n      <td>67.0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>930 rows √ó 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Load CLIP and define prompts","metadata":{}},{"cell_type":"code","source":"# Load CLIP model and define prompts\nprint(\"=\"*70)\nprint(\"LOADING CLIP MODEL\")\nprint(\"=\"*70)\n\n# Load CLIP\nmodel_clip, clip_preprocess = clip.load(CONFIG['clip_model'], device=device)\n\n# CRITICAL FIX: Force FP32 to prevent NaN\nmodel_clip = model_clip.float()\n\nprint(f\"‚úì Loaded CLIP {CONFIG['clip_model']}\")\nprint(f\"‚úì Model dtype: {next(model_clip.parameters()).dtype}\")\nprint(f\"‚úì Visual encoder parameters: {sum(p.numel() for p in model_clip.visual.parameters()):,}\")\n\n# Verify with test forward pass\ntest_img = torch.randn(2, 3, 224, 224).to(device)\nwith torch.no_grad():\n    test_output = model_clip.encode_image(test_img)\n    print(f\"‚úì Test forward pass: {test_output.shape}\")\n    if torch.isnan(test_output).any():\n        print(\"‚ùå NaN detected - restart kernel required\")\n    else:\n        print(\"‚úì No NaN - ready to train!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:48:02.917479Z","iopub.execute_input":"2025-11-26T07:48:02.918151Z","iopub.status.idle":"2025-11-26T07:48:07.213402Z","shell.execute_reply.started":"2025-11-26T07:48:02.918125Z","shell.execute_reply":"2025-11-26T07:48:07.212635Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nLOADING CLIP MODEL\n======================================================================\n‚úì Loaded CLIP ViT-B/32\n‚úì Model dtype: torch.float32\n‚úì Visual encoder parameters: 87,849,216\n‚úì Test forward pass: torch.Size([2, 512])\n‚úì No NaN - ready to train!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Define BI-RADS text prompts\nbirads_prompts = [\n    \"a mammogram showing BI-RADS category 1, normal breast tissue\",\n    \"a mammogram showing BI-RADS category 2, benign findings\",\n    \"a mammogram showing BI-RADS category 3, probably benign findings\",\n    \"a mammogram showing BI-RADS category 4, suspicious abnormality\"\n]\n\nprint(f\"‚úì Defined {len(birads_prompts)} text prompts for BI-RADS categories\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:48:30.767089Z","iopub.execute_input":"2025-11-26T07:48:30.767645Z","iopub.status.idle":"2025-11-26T07:48:30.771815Z","shell.execute_reply.started":"2025-11-26T07:48:30.767622Z","shell.execute_reply":"2025-11-26T07:48:30.770899Z"}},"outputs":[{"name":"stdout","text":"‚úì Defined 4 text prompts for BI-RADS categories\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Dataset class","metadata":{}},{"cell_type":"code","source":"# Contrastive Dataset class for CLIP fine-tuning\nclass BIRADSDataset(Dataset):\n    def __init__(self, dataframe, preprocess, text_prompts):\n        self.df = dataframe.reset_index(drop=True)\n        self.preprocess = preprocess\n        # Tokenize all prompts once\n        self.text_tokens = clip.tokenize(text_prompts)\n        print(f\"‚úì Tokenized {len(text_prompts)} text prompts\")\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        try:\n            # Load and preprocess image\n            img = Image.open(row['image_path']).convert('RGB')\n            img = self.preprocess(img)\n            \n            # Get corresponding text prompt\n            label = row['birads_label']  # 0, 1, 2, or 3\n            text = self.text_tokens[label]  # Matching text prompt\n            \n            return img, text, label\n        except Exception as e:\n            print(f\"Error loading {row['image_path']}: {e}\")\n            return None\n\n# Filtered dataset to skip None values\nclass FilteredDataset(Dataset):\n    def __init__(self, base_dataset):\n        self.base_ds = base_dataset\n        self.valid_idx = [i for i in range(len(base_dataset)) \n                         if base_dataset[i] is not None]\n        print(f\"‚úì Valid samples: {len(self.valid_idx)}/{len(base_dataset)}\")\n    \n    def __len__(self):\n        return len(self.valid_idx)\n    \n    def __getitem__(self, idx):\n        return self.base_ds[self.valid_idx[idx]]\n\nprint(\"‚úì Contrastive dataset classes defined\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:48:56.888547Z","iopub.execute_input":"2025-11-26T07:48:56.889174Z","iopub.status.idle":"2025-11-26T07:48:56.897053Z","shell.execute_reply.started":"2025-11-26T07:48:56.889149Z","shell.execute_reply":"2025-11-26T07:48:56.896205Z"}},"outputs":[{"name":"stdout","text":"‚úì Contrastive dataset classes defined\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Prepare data splits with contrastive learning\nprint(\"=\"*70)\nprint(\"PREPARING DATA SPLITS\")\nprint(\"=\"*70)\n\n# Split data\ntrain_df_5class, val_df_5class = train_test_split(\n    df, \n    test_size=0.2, \n    stratify=df['birads_label'], \n    random_state=CONFIG['seed']\n)\n\nprint(f\"Split: Train={len(train_df_5class)}, Val={len(val_df_5class)}\")\n\n# Create datasets with text prompts\ntrain_ds_5class = FilteredDataset(\n    BIRADSDataset(train_df_5class, preprocess=clip_preprocess, text_prompts=birads_prompts)\n)\nval_ds_5class = FilteredDataset(\n    BIRADSDataset(val_df_5class, preprocess=clip_preprocess, text_prompts=birads_prompts)\n)\n\n# Create DataLoaders\ntrain_loader_5class = DataLoader(\n    train_ds_5class, \n    batch_size=CONFIG['batch_size'], \n    shuffle=True, \n    num_workers=2\n)\nval_loader_5class = DataLoader(\n    val_ds_5class, \n    batch_size=CONFIG['batch_size'], \n    num_workers=2\n)\n\nprint(f\"‚úì Dataloaders ready: Train={len(train_ds_5class)}, Val={len(val_ds_5class)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:49:21.633540Z","iopub.execute_input":"2025-11-26T07:49:21.633778Z","iopub.status.idle":"2025-11-26T07:50:11.784576Z","shell.execute_reply.started":"2025-11-26T07:49:21.633763Z","shell.execute_reply":"2025-11-26T07:50:11.783861Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nPREPARING DATA SPLITS\n======================================================================\nSplit: Train=744, Val=186\n‚úì Tokenized 4 text prompts\n‚úì Valid samples: 744/744\n‚úì Tokenized 4 text prompts\n‚úì Valid samples: 186/186\n‚úì Dataloaders ready: Train=744, Val=186\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# Save pre-trained baseline","metadata":{}},{"cell_type":"code","source":"# Save pretrained CLIP visual encoder\nprint(\"=\"*70)\nprint(\"SAVING PRETRAINED BASELINE (CLIP visual encoder)\")\nprint(\"=\"*70)\n\ntorch.save(model_clip.visual.state_dict(), 'clip_visual_pretrained.pt')\n\nprint(f\"‚úì Saved pretrained CLIP {CONFIG['clip_model']} visual encoder\")\nprint(f\"  Parameters: {sum(p.numel() for p in model_clip.visual.parameters()):,}\")\n\nempty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:43:18.056236Z","iopub.execute_input":"2025-11-26T07:43:18.056451Z","iopub.status.idle":"2025-11-26T07:43:18.446878Z","shell.execute_reply.started":"2025-11-26T07:43:18.056435Z","shell.execute_reply":"2025-11-26T07:43:18.446141Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nSAVING PRETRAINED BASELINE (CLIP visual encoder)\n======================================================================\n‚úì Saved pretrained CLIP ViT-B/32 visual encoder\n  Parameters: 87,849,216\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Prepare data for 5-class training\n","metadata":{}},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:43:18.447812Z","iopub.execute_input":"2025-11-26T07:43:18.448090Z","iopub.status.idle":"2025-11-26T07:43:18.465338Z","shell.execute_reply.started":"2025-11-26T07:43:18.448067Z","shell.execute_reply":"2025-11-26T07:43:18.464614Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"    image_id                                         image_path  \\\n0       4167  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n1       0144  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n2       1850  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n3       0003  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n4       2010  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n..       ...                                                ...   \n925     0103  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n926     0031  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n927     0598  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n928     0271  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n929     1911  /kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...   \n\n    classification  birads   age  status  birads_label  \n0           Benign       3  44.0  Benign             2  \n1           Cancer       3  74.0  Cancer             2  \n2           Cancer       1  82.0  Cancer             0  \n3           Normal       2  46.0  Cancer             1  \n4           Normal       1  74.0  Normal             0  \n..             ...     ...   ...     ...           ...  \n925         Cancer       1  50.0  Cancer             0  \n926         Normal       1  81.0  Cancer             0  \n927         Normal       3  52.0  Normal             2  \n928         Normal       2  53.0  Benign             1  \n929         Benign       1  67.0  Benign             0  \n\n[930 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>image_path</th>\n      <th>classification</th>\n      <th>birads</th>\n      <th>age</th>\n      <th>status</th>\n      <th>birads_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4167</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Benign</td>\n      <td>3</td>\n      <td>44.0</td>\n      <td>Benign</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0144</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Cancer</td>\n      <td>3</td>\n      <td>74.0</td>\n      <td>Cancer</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1850</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Cancer</td>\n      <td>1</td>\n      <td>82.0</td>\n      <td>Cancer</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0003</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Normal</td>\n      <td>2</td>\n      <td>46.0</td>\n      <td>Cancer</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Normal</td>\n      <td>1</td>\n      <td>74.0</td>\n      <td>Normal</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>925</th>\n      <td>0103</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Cancer</td>\n      <td>1</td>\n      <td>50.0</td>\n      <td>Cancer</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>926</th>\n      <td>0031</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Normal</td>\n      <td>1</td>\n      <td>81.0</td>\n      <td>Cancer</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>927</th>\n      <td>0598</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Normal</td>\n      <td>3</td>\n      <td>52.0</td>\n      <td>Normal</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>928</th>\n      <td>0271</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Normal</td>\n      <td>2</td>\n      <td>53.0</td>\n      <td>Benign</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>929</th>\n      <td>1911</td>\n      <td>/kaggle/input/miniddsm2/MINI-DDSM-Complete-JPE...</td>\n      <td>Benign</td>\n      <td>1</td>\n      <td>67.0</td>\n      <td>Benign</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>930 rows √ó 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# Train Base Model (5-class on imbalanced data)","metadata":{}},{"cell_type":"markdown","source":"# Dry run: without freezing","metadata":{}},{"cell_type":"code","source":"# Train Base Model with Contrastive Loss - NO FREEZING\nprint(\"=\"*70)\nprint(\"TRAINING CLIP WITH CONTRASTIVE LOSS - ALL LAYERS TRAINABLE\")\nprint(\"=\"*70)\n\n# Make ALL parameters trainable (no freezing)\nfor param in model_clip.parameters():\n    param.requires_grad = True\n\nprint(\"‚úì ALL layers trainable (visual + text encoder)\")\nprint(f\"‚úì Total trainable parameters: {sum(p.numel() for p in model_clip.parameters() if p.requires_grad):,}\")\n\n# Optimizer - training everything\noptimizer_base = optim.AdamW(\n    model_clip.parameters(),  # Train ALL parameters\n    lr=1e-6,  # Conservative LR\n    weight_decay=1e-4,\n    eps=1e-8\n)\n\nscheduler = optim.lr_scheduler.CosineAnnealingLR(\n    optimizer_base, \n    T_max=CONFIG['epochs']\n)\n\nbest_acc_base = 0\n\nfor epoch in range(CONFIG['epochs']):\n    print(f\"\\n{'='*70}\")\n    print(f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n    print(f\"{'='*70}\")\n    \n    # ========== TRAINING ==========\n    model_clip.train()\n    train_loss = 0\n    num_batches = 0\n    nan_count = 0\n    \n    for batch_idx, (images, texts, labels) in enumerate(tqdm(train_loader_5class, desc=\"Training\", leave=False)):\n        images = images.to(device).float()  # Force FP32\n        texts = texts.to(device)\n        labels = labels.to(device)\n        \n        # Check inputs\n        if torch.isnan(images).any() or torch.isinf(images).any():\n            print(f\"\\n‚ö† NaN/Inf in input images batch {batch_idx}\")\n            nan_count += 1\n            continue\n        \n        optimizer_base.zero_grad()\n        \n        # Encode image and text\n        image_features = model_clip.encode_image(images)\n        text_features = model_clip.encode_text(texts)\n        \n        # Check for NaN after encoding\n        if torch.isnan(image_features).any() or torch.isnan(text_features).any():\n            print(f\"\\n‚ö† NaN after encoding batch {batch_idx}\")\n            nan_count += 1\n            if nan_count > 5:\n                print(\"‚ùå Too many NaN batches, stopping epoch\")\n                break\n            continue\n        \n        # Normalize features\n        image_features = image_features / (image_features.norm(dim=-1, keepdim=True) + 1e-8)\n        text_features = text_features / (text_features.norm(dim=-1, keepdim=True) + 1e-8)\n        \n        # Compute similarity (contrastive)\n        logit_scale = model_clip.logit_scale.exp().clamp(max=100)\n        logits_per_image = logit_scale * image_features @ text_features.T\n        logits_per_text = logits_per_image.T\n        \n        # Contrastive loss: each image should match its corresponding text\n        batch_size = images.shape[0]\n        target = torch.arange(batch_size).to(device)\n        \n        loss_img = nn.CrossEntropyLoss()(logits_per_image, target)\n        loss_txt = nn.CrossEntropyLoss()(logits_per_text, target)\n        loss = (loss_img + loss_txt) / 2\n        \n        # Check loss\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\"\\n‚ö† NaN/Inf loss batch {batch_idx}\")\n            nan_count += 1\n            continue\n        \n        loss.backward()\n        \n        # Gradient clipping\n        grad_norm = torch.nn.utils.clip_grad_norm_(\n            model_clip.parameters(),  # Clip all gradients\n            max_norm=1.0\n        )\n        \n        optimizer_base.step()\n        \n        train_loss += loss.item()\n        num_batches += 1\n        \n        # Progress check every 50 batches\n        if batch_idx % 50 == 0 and batch_idx > 0:\n            print(f\"\\n  Batch {batch_idx}: loss={loss.item():.4f}, grad_norm={grad_norm:.4f}\")\n    \n    scheduler.step()\n    \n    if nan_count > 5:\n        print(f\"\\n‚ùå Epoch {epoch+1} failed, stopping training\")\n        break\n    \n    # ========== VALIDATION ==========\n    model_clip.eval()\n    all_preds, all_labels = [], []\n    \n    # Encode all text prompts once for zero-shot evaluation\n    with torch.no_grad():\n        text_tokens_all = clip.tokenize(birads_prompts).to(device)\n        text_features_all = model_clip.encode_text(text_tokens_all)\n        text_features_all = text_features_all / (text_features_all.norm(dim=-1, keepdim=True) + 1e-8)\n    \n    with torch.no_grad():\n        for images, texts, labels in val_loader_5class:\n            images = images.to(device).float()\n            labels = labels.to(device)\n            \n            # Encode images\n            image_features = model_clip.encode_image(images)\n            \n            # Skip if NaN\n            if torch.isnan(image_features).any():\n                print(\"‚ö† NaN in validation batch\")\n                continue\n            \n            # Normalize\n            image_features = image_features / (image_features.norm(dim=-1, keepdim=True) + 1e-8)\n            \n            # Compare with ALL text prompts (zero-shot style)\n            logit_scale = model_clip.logit_scale.exp().clamp(max=100)\n            logits = logit_scale * image_features @ text_features_all.T\n            \n            # Predict class with highest similarity\n            preds = logits.argmax(dim=1)\n            \n            all_preds.extend(preds.cpu().tolist())\n            all_labels.extend(labels.cpu().tolist())\n    \n    if len(all_preds) == 0:\n        print(f\"\\n‚ö† No valid predictions in epoch {epoch+1}\")\n        continue\n    \n    # Compute metrics\n    acc = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n    avg_loss = train_loss / num_batches if num_batches > 0 else float('nan')\n    \n    print(f\"\\nüìä Epoch {epoch+1} Results:\")\n    print(f\"   Loss: {avg_loss:.4f}\")\n    print(f\"   Accuracy: {acc:.1f}%\")\n    print(f\"   LR: {scheduler.get_last_lr()[0]:.2e}\")\n    print(f\"   Valid batches: {num_batches}, NaN skipped: {nan_count}\")\n    \n    # Save best model\n    if acc > best_acc_base:\n        best_acc_base = acc\n        torch.save(model_clip.state_dict(), 'clip_full_imbalanced.pt')  # Save full model\n        print(f\"   ‚úì Saved best model! (Acc: {best_acc_base:.1f}%)\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"Training complete - Best accuracy: {best_acc_base:.1f}%\")\nprint(f\"{'='*70}\")\n\n# Final classification report\nif len(all_preds) > 0:\n    print(\"\\nüìä Final Classification Report:\")\n    print(classification_report(\n        all_labels, all_preds, \n        target_names=['BI-RADS 1', 'BI-RADS 2', 'BI-RADS 3', 'BI-RADS 4'],\n        zero_division=0\n    ))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:51:34.411095Z","iopub.execute_input":"2025-11-26T07:51:34.411724Z","iopub.status.idle":"2025-11-26T07:53:15.949752Z","shell.execute_reply.started":"2025-11-26T07:51:34.411702Z","shell.execute_reply":"2025-11-26T07:53:15.948758Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nTRAINING CLIP WITH CONTRASTIVE LOSS - ALL LAYERS TRAINABLE\n======================================================================\n‚úì ALL layers trainable (visual + text encoder)\n‚úì Total trainable parameters: 151,277,313\n\n======================================================================\nEpoch 1/3\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Training:  28%|‚ñà‚ñà‚ñä       | 52/186 [00:08<00:19,  6.92it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 50: loss=1.4721, grad_norm=64.8021\n","output_type":"stream"},{"name":"stderr","text":"Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 102/186 [00:15<00:11,  7.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 100: loss=1.2125, grad_norm=106.1445\n","output_type":"stream"},{"name":"stderr","text":"Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 152/186 [00:22<00:04,  7.08it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 150: loss=1.2007, grad_norm=162.3865\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 1 Results:\n   Loss: 1.3158\n   Accuracy: 50.0%\n   LR: 7.50e-07\n   Valid batches: 186, NaN skipped: 0\n   ‚úì Saved best model! (Acc: 50.0%)\n\n======================================================================\nEpoch 2/3\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Training:  28%|‚ñà‚ñà‚ñä       | 52/186 [00:07<00:19,  6.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 50: loss=1.8829, grad_norm=158.2827\n","output_type":"stream"},{"name":"stderr","text":"Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 102/186 [00:15<00:11,  7.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 100: loss=0.9552, grad_norm=74.6379\n","output_type":"stream"},{"name":"stderr","text":"Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 152/186 [00:22<00:04,  7.31it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 150: loss=1.6171, grad_norm=138.9697\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 2 Results:\n   Loss: 1.1210\n   Accuracy: 47.3%\n   LR: 2.50e-07\n   Valid batches: 186, NaN skipped: 0\n\n======================================================================\nEpoch 3/3\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Training:  28%|‚ñà‚ñà‚ñä       | 52/186 [00:07<00:19,  6.89it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 50: loss=0.8641, grad_norm=136.5248\n","output_type":"stream"},{"name":"stderr","text":"Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 102/186 [00:15<00:12,  6.54it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 100: loss=1.0767, grad_norm=129.6267\n","output_type":"stream"},{"name":"stderr","text":"Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 152/186 [00:22<00:04,  7.25it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 150: loss=0.8671, grad_norm=107.8156\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 3 Results:\n   Loss: 0.9571\n   Accuracy: 52.7%\n   LR: 0.00e+00\n   Valid batches: 186, NaN skipped: 0\n   ‚úì Saved best model! (Acc: 52.7%)\n\n======================================================================\nTraining complete - Best accuracy: 52.7%\n======================================================================\n\nüìä Final Classification Report:\n              precision    recall  f1-score   support\n\n   BI-RADS 1       0.63      0.55      0.59        60\n   BI-RADS 2       0.55      0.55      0.55        60\n   BI-RADS 3       0.62      0.48      0.54        60\n   BI-RADS 4       0.11      0.50      0.18         6\n\n    accuracy                           0.53       186\n   macro avg       0.48      0.52      0.47       186\nweighted avg       0.58      0.53      0.55       186\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Actual run: With freezing","metadata":{}},{"cell_type":"code","source":"# Train Base Model - TASK VECTOR FRIENDLY (Only Visual Encoder)\nprint(\"=\"*70)\nprint(\"TRAINING CLIP - TASK VECTOR FRIENDLY\")\nprint(\"FREEZING: Text encoder\")\nprint(\"TRAINING: Visual encoder only\")\nprint(\"=\"*70)\n\n# === FREEZE TEXT ENCODER ===\nfor param in model_clip.transformer.parameters():\n    param.requires_grad = False\nfor param in model_clip.token_embedding.parameters():\n    param.requires_grad = False\nfor param in model_clip.ln_final.parameters():\n    param.requires_grad = False\n\n# Fix: positional_embedding is a single tensor, not a list\nmodel_clip.positional_embedding.requires_grad = False\n\nprint(\"‚úì Text encoder FROZEN\")\n\n# === UNFREEZE VISUAL ENCODER ===\nfor param in model_clip.visual.parameters():\n    param.requires_grad = True\n\nprint(\"‚úì Visual encoder TRAINABLE\")\n\n# Count parameters\ntrainable_params = sum(p.numel() for p in model_clip.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model_clip.parameters())\nprint(f\"‚úì Trainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.1f}%)\")\n\n# Optimizer - only visual encoder parameters\noptimizer_base = optim.AdamW(\n    [p for p in model_clip.visual.parameters() if p.requires_grad],\n    lr=1e-6,  # Conservative LR\n    weight_decay=1e-4,\n    eps=1e-8\n)\n\nscheduler = optim.lr_scheduler.CosineAnnealingLR(\n    optimizer_base, \n    T_max=CONFIG['epochs']\n)\n\nbest_acc_base = 0\n\nfor epoch in range(CONFIG['epochs']):\n    print(f\"\\n{'='*70}\")\n    print(f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n    print(f\"{'='*70}\")\n    \n    # ========== TRAINING ==========\n    model_clip.train()\n    # Keep text encoder in eval mode\n    model_clip.transformer.eval()\n    \n    train_loss = 0\n    num_batches = 0\n    nan_count = 0\n    \n    for batch_idx, (images, texts, labels) in enumerate(tqdm(train_loader_5class, desc=\"Training\", leave=False)):\n        images = images.to(device).float()\n        texts = texts.to(device)\n        labels = labels.to(device)\n        \n        # Check inputs\n        if torch.isnan(images).any() or torch.isinf(images).any():\n            print(f\"\\n‚ö† NaN/Inf in input images batch {batch_idx}\")\n            nan_count += 1\n            continue\n        \n        optimizer_base.zero_grad()\n        \n        # Encode image (trainable)\n        image_features = model_clip.encode_image(images)\n        \n        # Encode text (frozen, no grad)\n        with torch.no_grad():\n            text_features = model_clip.encode_text(texts)\n        \n        # Check for NaN after encoding\n        if torch.isnan(image_features).any() or torch.isnan(text_features).any():\n            print(f\"\\n‚ö† NaN after encoding batch {batch_idx}\")\n            nan_count += 1\n            if nan_count > 5:\n                print(\"‚ùå Too many NaN batches, stopping epoch\")\n                break\n            continue\n        \n        # Normalize features\n        image_features = image_features / (image_features.norm(dim=-1, keepdim=True) + 1e-8)\n        text_features = text_features / (text_features.norm(dim=-1, keepdim=True) + 1e-8)\n        \n        # Compute similarity (contrastive)\n        logit_scale = model_clip.logit_scale.exp().clamp(max=100)\n        logits_per_image = logit_scale * image_features @ text_features.T\n        logits_per_text = logits_per_image.T\n        \n        # Contrastive loss\n        batch_size = images.shape[0]\n        target = torch.arange(batch_size).to(device)\n        \n        loss_img = nn.CrossEntropyLoss()(logits_per_image, target)\n        loss_txt = nn.CrossEntropyLoss()(logits_per_text, target)\n        loss = (loss_img + loss_txt) / 2\n        \n        # Check loss\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\"\\n‚ö† NaN/Inf loss batch {batch_idx}\")\n            nan_count += 1\n            continue\n        \n        loss.backward()\n        \n        # Gradient clipping\n        grad_norm = torch.nn.utils.clip_grad_norm_(\n            model_clip.visual.parameters(),\n            max_norm=1.0\n        )\n        \n        optimizer_base.step()\n        \n        train_loss += loss.item()\n        num_batches += 1\n        \n        # Progress check\n        if batch_idx % 50 == 0 and batch_idx > 0:\n            print(f\"\\n  Batch {batch_idx}: loss={loss.item():.4f}, grad_norm={grad_norm:.4f}\")\n    \n    scheduler.step()\n    \n    if nan_count > 5:\n        print(f\"\\n‚ùå Epoch {epoch+1} failed, stopping training\")\n        break\n    \n    # ========== VALIDATION ==========\n    model_clip.eval()\n    all_preds, all_labels = [], []\n    \n    # Encode all text prompts once\n    with torch.no_grad():\n        text_tokens_all = clip.tokenize(birads_prompts).to(device)\n        text_features_all = model_clip.encode_text(text_tokens_all)\n        text_features_all = text_features_all / (text_features_all.norm(dim=-1, keepdim=True) + 1e-8)\n    \n    with torch.no_grad():\n        for images, texts, labels in val_loader_5class:\n            images = images.to(device).float()\n            labels = labels.to(device)\n            \n            # Encode images\n            image_features = model_clip.encode_image(images)\n            \n            if torch.isnan(image_features).any():\n                print(\"‚ö† NaN in validation batch\")\n                continue\n            \n            # Normalize\n            image_features = image_features / (image_features.norm(dim=-1, keepdim=True) + 1e-8)\n            \n            # Compare with all text prompts\n            logit_scale = model_clip.logit_scale.exp().clamp(max=100)\n            logits = logit_scale * image_features @ text_features_all.T\n            \n            preds = logits.argmax(dim=1)\n            \n            all_preds.extend(preds.cpu().tolist())\n            all_labels.extend(labels.cpu().tolist())\n    \n    if len(all_preds) == 0:\n        print(f\"\\n‚ö† No valid predictions in epoch {epoch+1}\")\n        continue\n    \n    # Compute metrics\n    acc = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n    avg_loss = train_loss / num_batches if num_batches > 0 else float('nan')\n    \n    print(f\"\\nüìä Epoch {epoch+1} Results:\")\n    print(f\"   Loss: {avg_loss:.4f}\")\n    print(f\"   Accuracy: {acc:.1f}%\")\n    print(f\"   LR: {scheduler.get_last_lr()[0]:.2e}\")\n    print(f\"   Valid batches: {num_batches}, NaN skipped: {nan_count}\")\n    \n    # Save best model\n    if acc > best_acc_base:\n        best_acc_base = acc\n        torch.save(model_clip.visual.state_dict(), 'clip_visual_imbalanced.pt')\n        print(f\"   ‚úì Saved best visual encoder! (Acc: {best_acc_base:.1f}%)\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"Training complete - Best accuracy: {best_acc_base:.1f}%\")\nprint(f\"{'='*70}\")\n\n# Final classification report\nif len(all_preds) > 0:\n    print(\"\\nüìä Final Classification Report:\")\n    print(classification_report(\n        all_labels, all_preds, \n        target_names=['BI-RADS 1', 'BI-RADS 2', 'BI-RADS 3', 'BI-RADS 4'],\n        zero_division=0\n    ))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:58:49.330015Z","iopub.execute_input":"2025-11-26T07:58:49.330600Z","iopub.status.idle":"2025-11-26T08:00:24.078320Z","shell.execute_reply.started":"2025-11-26T07:58:49.330578Z","shell.execute_reply":"2025-11-26T08:00:24.077178Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nTRAINING CLIP - TASK VECTOR FRIENDLY\nFREEZING: Text encoder\nTRAINING: Visual encoder only\n======================================================================\n‚úì Text encoder FROZEN\n‚úì Visual encoder TRAINABLE\n‚úì Trainable: 88,111,361 / 151,277,313 (58.2%)\n\n======================================================================\nEpoch 1/3\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Training:  27%|‚ñà‚ñà‚ñã       | 51/186 [00:07<00:20,  6.73it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 50: loss=1.1265, grad_norm=169.1553\n","output_type":"stream"},{"name":"stderr","text":"Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 101/186 [00:14<00:12,  6.69it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 100: loss=0.5116, grad_norm=60.4248\n","output_type":"stream"},{"name":"stderr","text":"Training:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 151/186 [00:21<00:04,  7.60it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 150: loss=0.5908, grad_norm=94.3668\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 1 Results:\n   Loss: 0.9457\n   Accuracy: 50.5%\n   LR: 7.50e-07\n   Valid batches: 186, NaN skipped: 0\n   ‚úì Saved best visual encoder! (Acc: 50.5%)\n\n======================================================================\nEpoch 2/3\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Training:  27%|‚ñà‚ñà‚ñã       | 51/186 [00:07<00:21,  6.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 50: loss=0.7731, grad_norm=199.4700\n","output_type":"stream"},{"name":"stderr","text":"Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 101/186 [00:14<00:10,  8.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 100: loss=0.9560, grad_norm=180.9445\n","output_type":"stream"},{"name":"stderr","text":"Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 152/186 [00:20<00:04,  7.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 150: loss=0.4581, grad_norm=60.5807\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 2 Results:\n   Loss: 0.8270\n   Accuracy: 51.6%\n   LR: 2.50e-07\n   Valid batches: 186, NaN skipped: 0\n   ‚úì Saved best visual encoder! (Acc: 51.6%)\n\n======================================================================\nEpoch 3/3\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Training:  27%|‚ñà‚ñà‚ñã       | 51/186 [00:07<00:18,  7.23it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 50: loss=0.3200, grad_norm=170.7865\n","output_type":"stream"},{"name":"stderr","text":"Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 102/186 [00:13<00:09,  8.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 100: loss=0.8712, grad_norm=147.4239\n","output_type":"stream"},{"name":"stderr","text":"Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 152/186 [00:20<00:04,  7.35it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Batch 150: loss=0.5316, grad_norm=157.7780\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 3 Results:\n   Loss: 0.7345\n   Accuracy: 55.4%\n   LR: 0.00e+00\n   Valid batches: 186, NaN skipped: 0\n   ‚úì Saved best visual encoder! (Acc: 55.4%)\n\n======================================================================\nTraining complete - Best accuracy: 55.4%\n======================================================================\n\nüìä Final Classification Report:\n              precision    recall  f1-score   support\n\n   BI-RADS 1       0.59      0.67      0.62        60\n   BI-RADS 2       0.55      0.52      0.53        60\n   BI-RADS 3       0.67      0.50      0.57        60\n   BI-RADS 4       0.12      0.33      0.17         6\n\n    accuracy                           0.55       186\n   macro avg       0.48      0.50      0.48       186\nweighted avg       0.59      0.55      0.56       186\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# Create task vector birads ( 1 to 5)","metadata":{}},{"cell_type":"code","source":"pretrained_sd = torch.load('clip_visual_pretrained.pt', weights_only=False)\nfinetuned_sd = torch.load('clip_visual_imbalanced.pt', weights_only=False)\n\n# Create the vector dictionary: finetuned - pretrained\nvector_dict = {}\nfor key in pretrained_sd.keys():\n    if key in finetuned_sd and pretrained_sd[key].shape == finetuned_sd[key].shape:\n        vector_dict[key] = finetuned_sd[key] - pretrained_sd[key]\n\nprint(f\"‚úì Created vector dict with {len(vector_dict)} parameter groups\")\n\n\n# Create TaskVector directly with the vector parameter\ntask_vector = TaskVector(vector=vector_dict)\n\nprint(task_vector)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:04:33.263711Z","iopub.execute_input":"2025-11-26T08:04:33.264415Z","iopub.status.idle":"2025-11-26T08:04:33.557874Z","shell.execute_reply.started":"2025-11-26T08:04:33.264390Z","shell.execute_reply":"2025-11-26T08:04:33.557131Z"}},"outputs":[{"name":"stdout","text":"‚úì Created vector dict with 152 parameter groups\n<task_vectors.TaskVector object at 0x7d09f27a40d0>\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Quick inspection\nprint(f\"Task vector has {len(task_vector.vector)} parameter groups\")\nprint(f\"\\nParameter names:\")\nfor i, key in enumerate(list(task_vector.vector.keys())[:10]):  # First 10\n    shape = task_vector.vector[key].shape\n    print(f\"  {i+1}. {key}: {shape}\")\nprint(\"...\")\n\n# Pick one parameter to examine\nsample_key = list(task_vector.vector.keys())[0]\nsample_tensor = task_vector.vector[sample_key]\nprint(f\"\\nExample parameter '{sample_key}':\")\nprint(f\"  Shape: {sample_tensor.shape}\")\nprint(f\"  First 5 values: {sample_tensor.flatten()[:5].tolist()}\")\nprint(f\"  Mean change: {sample_tensor.mean().item():.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:05:34.002396Z","iopub.execute_input":"2025-11-26T08:05:34.003148Z","iopub.status.idle":"2025-11-26T08:05:34.009575Z","shell.execute_reply.started":"2025-11-26T08:05:34.003124Z","shell.execute_reply":"2025-11-26T08:05:34.008788Z"}},"outputs":[{"name":"stdout","text":"Task vector has 152 parameter groups\n\nParameter names:\n  1. class_embedding: torch.Size([768])\n  2. positional_embedding: torch.Size([50, 768])\n  3. proj: torch.Size([768, 512])\n  4. conv1.weight: torch.Size([768, 3, 32, 32])\n  5. ln_pre.weight: torch.Size([768])\n  6. ln_pre.bias: torch.Size([768])\n  7. transformer.resblocks.0.attn.in_proj_weight: torch.Size([2304, 768])\n  8. transformer.resblocks.0.attn.in_proj_bias: torch.Size([2304])\n  9. transformer.resblocks.0.attn.out_proj.weight: torch.Size([768, 768])\n  10. transformer.resblocks.0.attn.out_proj.bias: torch.Size([768])\n...\n\nExample parameter 'class_embedding':\n  Shape: torch.Size([768])\n  First 5 values: [-1.0201707482337952e-05, 2.5631568860262632e-05, -1.4337245374917984e-05, 1.498498022556305e-06, -1.2629665434360504e-05]\n  Mean change: 0.00000606\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"# BIRADS 1 Focus ","metadata":{}},{"cell_type":"code","source":"# Create BI-RADS 1 focused dataset\nprint(\"=\"*70)\nprint(\"CREATING BI-RADS 1 FOCUSED DATASET\")\nprint(\"=\"*70)\n\n# Create binary labels: 1 = BI-RADS 1, 0 = Not BI-RADS 1\ndf_birads1_focus = df.copy()\ndf_birads1_focus['is_birads1'] = (df_birads1_focus['birads'] == 1).astype(int)\n\nprint(f\"\\nBinary label distribution:\")\nprint(df_birads1_focus['is_birads1'].value_counts())\nprint(f\"  BI-RADS 1: {(df_birads1_focus['is_birads1'] == 1).sum()}\")\nprint(f\"  Not BI-RADS 1: {(df_birads1_focus['is_birads1'] == 0).sum()}\")\n\n# Split data\ntrain_df_birads1, val_df_birads1 = train_test_split(\n    df_birads1_focus, \n    test_size=0.2, \n    stratify=df_birads1_focus['is_birads1'], \n    random_state=CONFIG['seed']\n)\n\nprint(f\"\\n‚úì Split: Train={len(train_df_birads1)}, Val={len(val_df_birads1)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:16:01.860152Z","iopub.execute_input":"2025-11-26T08:16:01.861024Z","iopub.status.idle":"2025-11-26T08:16:01.874332Z","shell.execute_reply.started":"2025-11-26T08:16:01.860998Z","shell.execute_reply":"2025-11-26T08:16:01.873712Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nCREATING BI-RADS 1 FOCUSED DATASET\n======================================================================\n\nBinary label distribution:\nis_birads1\n0    630\n1    300\nName: count, dtype: int64\n  BI-RADS 1: 300\n  Not BI-RADS 1: 630\n\n‚úì Split: Train=744, Val=186\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Binary text prompts for BI-RADS 1 classification\nbirads1_prompts = [\n    \"a mammogram showing dense breast tissue, not BI-RADS category 1\",\n    \"a mammogram showing BI-RADS category 1, entirely fatty breast tissue\"\n]\n\nprint(f\"‚úì Defined {len(birads1_prompts)} binary prompts for BI-RADS 1 classification\")\nprint(f\"  0: Not BI-RADS 1\")\nprint(f\"  1: BI-RADS 1\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:17:12.943114Z","iopub.execute_input":"2025-11-26T08:17:12.943382Z","iopub.status.idle":"2025-11-26T08:17:12.948101Z","shell.execute_reply.started":"2025-11-26T08:17:12.943362Z","shell.execute_reply":"2025-11-26T08:17:12.947500Z"}},"outputs":[{"name":"stdout","text":"‚úì Defined 2 binary prompts for BI-RADS 1 classification\n  0: Not BI-RADS 1\n  1: BI-RADS 1\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Dataset class for BI-RADS 1 binary classification\nclass BIRADSBinaryDataset(Dataset):\n    def __init__(self, dataframe, preprocess, text_prompts):\n        self.df = dataframe.reset_index(drop=True)\n        self.preprocess = preprocess\n        self.text_tokens = clip.tokenize(text_prompts)\n        print(f\"‚úì Tokenized {len(text_prompts)} binary text prompts\")\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        try:\n            img = Image.open(row['image_path']).convert('RGB')\n            img = self.preprocess(img)\n            \n            label = row['is_birads1']  # 0 or 1\n            text = self.text_tokens[label]\n            \n            return img, text, label\n        except Exception as e:\n            print(f\"Error loading {row['image_path']}: {e}\")\n            return None\n\n# Create datasets\ntrain_ds_birads1 = FilteredDataset(\n    BIRADSBinaryDataset(train_df_birads1, preprocess=clip_preprocess, text_prompts=birads1_prompts)\n)\nval_ds_birads1 = FilteredDataset(\n    BIRADSBinaryDataset(val_df_birads1, preprocess=clip_preprocess, text_prompts=birads1_prompts)\n)\n\n# Create DataLoaders\ntrain_loader_birads1 = DataLoader(\n    train_ds_birads1, \n    batch_size=CONFIG['batch_size'], \n    shuffle=True, \n    num_workers=2\n)\nval_loader_birads1 = DataLoader(\n    val_ds_birads1, \n    batch_size=CONFIG['batch_size'], \n    num_workers=2\n)\n\nprint(f\"‚úì BI-RADS 1 dataloaders ready: Train={len(train_ds_birads1)}, Val={len(val_ds_birads1)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:17:16.094010Z","iopub.execute_input":"2025-11-26T08:17:16.094270Z","iopub.status.idle":"2025-11-26T08:18:05.992024Z","shell.execute_reply.started":"2025-11-26T08:17:16.094251Z","shell.execute_reply":"2025-11-26T08:18:05.991161Z"}},"outputs":[{"name":"stdout","text":"‚úì Tokenized 2 binary text prompts\n‚úì Valid samples: 744/744\n‚úì Tokenized 2 binary text prompts\n‚úì Valid samples: 186/186\n‚úì BI-RADS 1 dataloaders ready: Train=744, Val=186\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# Train BI-RADS 1 Focused Model\nprint(\"=\"*70)\nprint(\"TRAINING CLIP - BI-RADS 1 FOCUSED\")\nprint(\"=\"*70)\n\n# Reload CLIP fresh (reset to pretrained state)\ndel model_clip\nempty_cache()\n\nmodel_clip, _ = clip.load(CONFIG['clip_model'], device=device)\nmodel_clip = model_clip.float()\n\nprint(f\"‚úì Reloaded fresh CLIP {CONFIG['clip_model']}\")\n\n# Freeze text encoder\nfor param in model_clip.transformer.parameters():\n    param.requires_grad = False\nfor param in model_clip.token_embedding.parameters():\n    param.requires_grad = False\nfor param in model_clip.ln_final.parameters():\n    param.requires_grad = False\nmodel_clip.positional_embedding.requires_grad = False\n\n# Unfreeze visual encoder\nfor param in model_clip.visual.parameters():\n    param.requires_grad = True\n\nprint(\"‚úì Text encoder FROZEN, Visual encoder TRAINABLE\")\n\n# Optimizer\noptimizer_birads1 = optim.AdamW(\n    [p for p in model_clip.visual.parameters() if p.requires_grad],\n    lr=1e-6,\n    weight_decay=1e-4,\n    eps=1e-8\n)\n\nscheduler_birads1 = optim.lr_scheduler.CosineAnnealingLR(\n    optimizer_birads1, \n    T_max=CONFIG['epochs']\n)\n\nbest_acc_birads1 = 0\n\nfor epoch in range(CONFIG['epochs']):\n    print(f\"\\n{'='*70}\")\n    print(f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n    print(f\"{'='*70}\")\n    \n    # Training\n    model_clip.train()\n    model_clip.transformer.eval()\n    \n    train_loss = 0\n    num_batches = 0\n    nan_count = 0\n    \n    for batch_idx, (images, texts, labels) in enumerate(tqdm(train_loader_birads1, desc=\"Training BI-RADS 1\", leave=False)):\n        images = images.to(device).float()\n        texts = texts.to(device)\n        labels = labels.to(device)\n        \n        if torch.isnan(images).any():\n            nan_count += 1\n            continue\n        \n        optimizer_birads1.zero_grad()\n        \n        image_features = model_clip.encode_image(images)\n        \n        with torch.no_grad():\n            text_features = model_clip.encode_text(texts)\n        \n        if torch.isnan(image_features).any():\n            nan_count += 1\n            if nan_count > 5:\n                break\n            continue\n        \n        image_features = image_features / (image_features.norm(dim=-1, keepdim=True) + 1e-8)\n        text_features = text_features / (text_features.norm(dim=-1, keepdim=True) + 1e-8)\n        \n        logit_scale = model_clip.logit_scale.exp().clamp(max=100)\n        logits_per_image = logit_scale * image_features @ text_features.T\n        logits_per_text = logits_per_image.T\n        \n        batch_size = images.shape[0]\n        target = torch.arange(batch_size).to(device)\n        \n        loss_img = nn.CrossEntropyLoss()(logits_per_image, target)\n        loss_txt = nn.CrossEntropyLoss()(logits_per_text, target)\n        loss = (loss_img + loss_txt) / 2\n        \n        if torch.isnan(loss):\n            nan_count += 1\n            continue\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model_clip.visual.parameters(), max_norm=1.0)\n        optimizer_birads1.step()\n        \n        train_loss += loss.item()\n        num_batches += 1\n    \n    scheduler_birads1.step()\n    \n    # Validation\n    model_clip.eval()\n    all_preds, all_labels = [], []\n    \n    with torch.no_grad():\n        text_tokens_all = clip.tokenize(birads1_prompts).to(device)\n        text_features_all = model_clip.encode_text(text_tokens_all)\n        text_features_all = text_features_all / (text_features_all.norm(dim=-1, keepdim=True) + 1e-8)\n    \n    with torch.no_grad():\n        for images, texts, labels in val_loader_birads1:\n            images = images.to(device).float()\n            labels = labels.to(device)\n            \n            image_features = model_clip.encode_image(images)\n            \n            if torch.isnan(image_features).any():\n                continue\n            \n            image_features = image_features / (image_features.norm(dim=-1, keepdim=True) + 1e-8)\n            \n            logit_scale = model_clip.logit_scale.exp().clamp(max=100)\n            logits = logit_scale * image_features @ text_features_all.T\n            \n            preds = logits.argmax(dim=1)\n            \n            all_preds.extend(preds.cpu().tolist())\n            all_labels.extend(labels.cpu().tolist())\n    \n    if len(all_preds) == 0:\n        continue\n    \n    acc = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n    avg_loss = train_loss / num_batches if num_batches > 0 else float('nan')\n    \n    print(f\"\\nüìä Epoch {epoch+1} Results:\")\n    print(f\"   Loss: {avg_loss:.4f}\")\n    print(f\"   Accuracy: {acc:.1f}%\")\n    print(f\"   LR: {scheduler_birads1.get_last_lr()[0]:.2e}\")\n    \n    if acc > best_acc_birads1:\n        best_acc_birads1 = acc\n        torch.save(model_clip.visual.state_dict(), 'clip_visual_birads1.pt')\n        print(f\"   ‚úì Saved! (Best: {best_acc_birads1:.1f}%)\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"BI-RADS 1 training complete - Best: {best_acc_birads1:.1f}%\")\nprint(f\"{'='*70}\")\n\nif len(all_preds) > 0:\n    print(\"\\nüìä Classification Report:\")\n    print(classification_report(\n        all_labels, all_preds, \n        target_names=['Not BI-RADS 1', 'BI-RADS 1'],\n        zero_division=0\n    ))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:18:31.662087Z","iopub.execute_input":"2025-11-26T08:18:31.662602Z","iopub.status.idle":"2025-11-26T08:20:11.935602Z","shell.execute_reply.started":"2025-11-26T08:18:31.662574Z","shell.execute_reply":"2025-11-26T08:20:11.934617Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nTRAINING CLIP - BI-RADS 1 FOCUSED\n======================================================================\n‚úì Reloaded fresh CLIP ViT-B/32\n‚úì Text encoder FROZEN, Visual encoder TRAINABLE\n\n======================================================================\nEpoch 1/3\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 1 Results:\n   Loss: 1.3878\n   Accuracy: 67.7%\n   LR: 7.50e-07\n   ‚úì Saved! (Best: 67.7%)\n\n======================================================================\nEpoch 2/3\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 2 Results:\n   Loss: 1.2665\n   Accuracy: 74.2%\n   LR: 2.50e-07\n   ‚úì Saved! (Best: 74.2%)\n\n======================================================================\nEpoch 3/3\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 3 Results:\n   Loss: 1.2078\n   Accuracy: 75.8%\n   LR: 0.00e+00\n   ‚úì Saved! (Best: 75.8%)\n\n======================================================================\nBI-RADS 1 training complete - Best: 75.8%\n======================================================================\n\nüìä Classification Report:\n               precision    recall  f1-score   support\n\nNot BI-RADS 1       0.81      0.84      0.82       126\n    BI-RADS 1       0.64      0.58      0.61        60\n\n     accuracy                           0.76       186\n    macro avg       0.72      0.71      0.72       186\n weighted avg       0.75      0.76      0.76       186\n\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"\npretrained_sd = torch.load('clip_visual_pretrained.pt', weights_only=False)\nbirads1_sd = torch.load('clip_visual_birads1.pt', weights_only=False)\n\n# Create vector dictionary\nbirads1_vector_dict = {}\nfor key in pretrained_sd.keys():\n    if key in birads1_sd and pretrained_sd[key].shape == birads1_sd[key].shape:\n        birads1_vector_dict[key] = birads1_sd[key] - pretrained_sd[key]\n\ntask_vector_birads1 = TaskVector(vector=birads1_vector_dict)\n\nprint(f\"‚úì BI-RADS 1 task vector created!\")\n\n# Quick inspection\ntotal_params = sum(v.numel() for v in task_vector_birads1.vector.values())\nprint(f\"‚úì Total parameters in task vector: {total_params:,}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:22:30.802352Z","iopub.execute_input":"2025-11-26T08:22:30.803000Z","iopub.status.idle":"2025-11-26T08:22:31.135936Z","shell.execute_reply.started":"2025-11-26T08:22:30.802975Z","shell.execute_reply":"2025-11-26T08:22:31.135293Z"}},"outputs":[{"name":"stdout","text":"‚úì BI-RADS 1 task vector created!\n‚úì Total parameters in task vector: 87,849,216\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"print(\"=\"*70)\nprint(\"COMPARING BI-RADS 1 TASK VECTORS\")\nprint(\"=\"*70)\n\n# Check they have the same structure\nprint(f\"\\nStructure check:\")\nprint(f\"  BI-RADS 1-5 vector: {len(task_vector.vector)} parameters\")\nprint(f\"  BI-RADS 1 focus vector: {len(task_vector_birads1.vector)} parameters\")\nprint(f\"  Keys match: {set(task_vector.vector.keys()) == set(task_vector_birads1.vector.keys())}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:25:16.975460Z","iopub.execute_input":"2025-11-26T08:25:16.976282Z","iopub.status.idle":"2025-11-26T08:25:16.981398Z","shell.execute_reply.started":"2025-11-26T08:25:16.976258Z","shell.execute_reply":"2025-11-26T08:25:16.980588Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nCOMPARING BI-RADS 1 TASK VECTORS\n======================================================================\n\nStructure check:\n  BI-RADS 1-5 vector: 152 parameters\n  BI-RADS 1 focus vector: 152 parameters\n  Keys match: True\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"# birads 5 focus","metadata":{}},{"cell_type":"code","source":"# BI-RADS 4 focused binary label\ndf_birads4_focus = df.copy()\ndf_birads4_focus['is_birads4'] = (df_birads4_focus['birads'] == 4).astype(int)\n\nprint(\"Binary label distribution:\")\nprint(df_birads4_focus['is_birads4'].value_counts())\n\n# Train/Val split\ntrain_df_birads4, val_df_birads4 = train_test_split(\n    df_birads4_focus,\n    test_size=0.2,\n    stratify=df_birads4_focus['is_birads4'],\n    random_state=CONFIG['seed']\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:26:16.224928Z","iopub.execute_input":"2025-11-26T08:26:16.225597Z","iopub.status.idle":"2025-11-26T08:26:16.236960Z","shell.execute_reply.started":"2025-11-26T08:26:16.225574Z","shell.execute_reply":"2025-11-26T08:26:16.236287Z"}},"outputs":[{"name":"stdout","text":"Binary label distribution:\nis_birads4\n0    900\n1     30\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"birads4_prompts = [\n    \"a normal or benign mammogram, not BI-RADS category 4\",  # Negative class\n    \"a mammogram showing BI-RADS category 4, suspicious abnormality\"  # Positive class\n]\n\nprint(f\"‚úì Defined {len(birads4_prompts)} prompts for BI-RADS 4 focus\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:26:27.510611Z","iopub.execute_input":"2025-11-26T08:26:27.510915Z","iopub.status.idle":"2025-11-26T08:26:27.515319Z","shell.execute_reply.started":"2025-11-26T08:26:27.510884Z","shell.execute_reply":"2025-11-26T08:26:27.514527Z"}},"outputs":[{"name":"stdout","text":"‚úì Defined 2 prompts for BI-RADS 4 focus\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"class BIRADSBinaryDataset(Dataset):\n    def __init__(self, dataframe, preprocess, text_prompts):\n        self.df = dataframe.reset_index(drop=True)\n        self.preprocess = preprocess\n        self.text_tokens = clip.tokenize(text_prompts)\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        try:\n            img = Image.open(row['image_path']).convert('RGB')\n            img = self.preprocess(img)\n            label = row['is_birads4']  # 0 or 1\n            text = self.text_tokens[label]\n            return img, text, label\n        except Exception as e:\n            print(f\"Error loading {row['image_path']}: {e}\")\n            return None\n\ntrain_ds_birads4 = FilteredDataset(\n    BIRADSBinaryDataset(train_df_birads4, preprocess=clip_preprocess, text_prompts=birads4_prompts)\n)\nval_ds_birads4 = FilteredDataset(\n    BIRADSBinaryDataset(val_df_birads4, preprocess=clip_preprocess, text_prompts=birads4_prompts)\n)\n\ntrain_loader_birads4 = DataLoader(train_ds_birads4, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\nval_loader_birads4 = DataLoader(val_ds_birads4, batch_size=CONFIG['batch_size'], num_workers=2)\n\nprint(f\"‚úì DataLoaders ready: Train={len(train_ds_birads4)}, Val={len(val_ds_birads4)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:26:38.828103Z","iopub.execute_input":"2025-11-26T08:26:38.828950Z","iopub.status.idle":"2025-11-26T08:27:28.959017Z","shell.execute_reply.started":"2025-11-26T08:26:38.828917Z","shell.execute_reply":"2025-11-26T08:27:28.958285Z"}},"outputs":[{"name":"stdout","text":"‚úì Valid samples: 744/744\n‚úì Valid samples: 186/186\n‚úì DataLoaders ready: Train=744, Val=186\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Reload CLIP (reset to pretrained)\ndel model_clip\nempty_cache()\nmodel_clip, _ = clip.load(CONFIG['clip_model'], device=device)\nmodel_clip = model_clip.float()\n\n# Freeze text encoder\nfor param in model_clip.transformer.parameters():\n    param.requires_grad = False\nfor param in model_clip.token_embedding.parameters():\n    param.requires_grad = False\nfor param in model_clip.ln_final.parameters():\n    param.requires_grad = False\nmodel_clip.positional_embedding.requires_grad = False\n\n# Unfreeze visual encoder\nfor param in model_clip.visual.parameters():\n    param.requires_grad = True\n\noptimizer_birads4 = optim.AdamW(\n    [p for p in model_clip.visual.parameters() if p.requires_grad],\n    lr=1e-6, weight_decay=1e-4, eps=1e-8)\n\nscheduler_birads4 = optim.lr_scheduler.CosineAnnealingLR(optimizer_birads4, T_max=CONFIG['epochs'])\n\nbest_acc_birads4 = 0\n\nfor epoch in range(CONFIG['epochs']):\n    print(f\"\\n--- Epoch {epoch+1}/{CONFIG['epochs']} ---\")\n    model_clip.train(); model_clip.transformer.eval()\n    train_loss, num_batches, nan_count = 0, 0, 0\n    \n    for images, texts, labels in tqdm(train_loader_birads4, desc=\"Training BI-RADS 4\", leave=False):\n        images, texts, labels = images.to(device).float(), texts.to(device), labels.to(device)\n        if torch.isnan(images).any(): nan_count += 1; continue\n        optimizer_birads4.zero_grad()\n\n        image_features = model_clip.encode_image(images)\n        with torch.no_grad():\n            text_features = model_clip.encode_text(texts)\n        if torch.isnan(image_features).any(): nan_count += 1; continue\n\n        image_features = image_features / (image_features.norm(dim=-1, keepdim=True) + 1e-8)\n        text_features = text_features / (text_features.norm(dim=-1, keepdim=True) + 1e-8)\n        logit_scale = model_clip.logit_scale.exp().clamp(max=100)\n        logits_per_image = logit_scale * image_features @ text_features.T\n        logits_per_text = logits_per_image.T\n\n        batch_size = images.shape[0]\n        target = torch.arange(batch_size).to(device)\n        loss_img = nn.CrossEntropyLoss()(logits_per_image, target)\n        loss_txt = nn.CrossEntropyLoss()(logits_per_text, target)\n        loss = (loss_img + loss_txt) / 2\n        if torch.isnan(loss): nan_count += 1; continue\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model_clip.visual.parameters(), max_norm=1.0)\n        optimizer_birads4.step()\n\n        train_loss += loss.item()\n        num_batches += 1\n    \n    scheduler_birads4.step()\n\n    # Validation\n    model_clip.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        text_tokens_all = clip.tokenize(birads4_prompts).to(device)\n        text_features_all = model_clip.encode_text(text_tokens_all)\n        text_features_all = text_features_all / (text_features_all.norm(dim=-1, keepdim=True) + 1e-8)\n\n        for images, texts, labels in val_loader_birads4:\n            images, labels = images.to(device).float(), labels.to(device)\n            image_features = model_clip.encode_image(images)\n            if torch.isnan(image_features).any(): continue\n            image_features = image_features / (image_features.norm(dim=-1, keepdim=True) + 1e-8)\n\n            logit_scale = model_clip.logit_scale.exp().clamp(max=100)\n            logits = logit_scale * image_features @ text_features_all.T\n            preds = logits.argmax(dim=1)\n\n            all_preds.extend(preds.cpu().tolist())\n            all_labels.extend(labels.cpu().tolist())\n    \n    if len(all_preds) == 0: continue\n    acc = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n    avg_loss = train_loss / num_batches if num_batches > 0 else float('nan')\n    print(f\"Loss: {avg_loss:.4f}, Accuracy: {acc:.1f}%\")\n    if acc > best_acc_birads4:\n        best_acc_birads4 = acc\n        torch.save(model_clip.visual.state_dict(), 'clip_visual_birads4.pt')\n        print(f\"   ‚úì Saved! (Best: {best_acc_birads4:.1f}%)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:27:31.366505Z","iopub.execute_input":"2025-11-26T08:27:31.366779Z","iopub.status.idle":"2025-11-26T08:29:10.286716Z","shell.execute_reply.started":"2025-11-26T08:27:31.366758Z","shell.execute_reply":"2025-11-26T08:29:10.285984Z"}},"outputs":[{"name":"stdout","text":"\n--- Epoch 1/3 ---\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Loss: 1.4063, Accuracy: 55.9%\n   ‚úì Saved! (Best: 55.9%)\n\n--- Epoch 2/3 ---\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Loss: 1.3762, Accuracy: 72.0%\n   ‚úì Saved! (Best: 72.0%)\n\n--- Epoch 3/3 ---\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Loss: 1.3643, Accuracy: 70.4%\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# Create BI-RADS 4 Task Vector\npretrained_sd = torch.load('clip_visual_pretrained.pt', weights_only=False)\nbirads4_sd = torch.load('clip_visual_birads4.pt', weights_only=False)\n\n# Create vector dictionary\nbirads4_vector_dict = {}\nfor key in pretrained_sd.keys():\n    if key in birads4_sd and pretrained_sd[key].shape == birads4_sd[key].shape:\n        birads4_vector_dict[key] = birads4_sd[key] - pretrained_sd[key]\n\ntask_vector_birads4 = TaskVector(vector=birads4_vector_dict)\n\nprint(f\"‚úì BI-RADS 4 task vector created!\")\n\n# Quick inspection\ntotal_params = sum(v.numel() for v in task_vector_birads1.vector.values())\nprint(f\"‚úì Total parameters in task vector: {total_params:,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:29:48.848759Z","iopub.execute_input":"2025-11-26T08:29:48.849092Z","iopub.status.idle":"2025-11-26T08:29:49.177111Z","shell.execute_reply.started":"2025-11-26T08:29:48.849065Z","shell.execute_reply":"2025-11-26T08:29:49.176300Z"}},"outputs":[{"name":"stdout","text":"‚úì BI-RADS 4 task vector created!\n‚úì Total parameters in task vector: 87,849,216\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"print(\"=\"*70)\nprint(\"COMPARING BI-RADS 1 TASK VECTORS\")\nprint(\"=\"*70)\n\n# Check they have the same structure\nprint(f\"\\nStructure check:\")\nprint(f\"  BI-RADS 1-5 vector: {len(task_vector.vector)} parameters\")\nprint(f\"  BI-RADS 1 focus vector: {len(task_vector_birads1.vector)} parameters\")\nprint(f\"  BI-RADS 4 focus vector: {len(task_vector_birads4.vector)} parameters\")\n\nprint(f\"  Keys match: {set(task_vector.vector.keys()) == set(task_vector_birads4.vector.keys())}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:30:36.288828Z","iopub.execute_input":"2025-11-26T08:30:36.289696Z","iopub.status.idle":"2025-11-26T08:30:36.294998Z","shell.execute_reply.started":"2025-11-26T08:30:36.289670Z","shell.execute_reply":"2025-11-26T08:30:36.294234Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nCOMPARING BI-RADS 1 TASK VECTORS\n======================================================================\n\nStructure check:\n  BI-RADS 1-5 vector: 152 parameters\n  BI-RADS 1 focus vector: 152 parameters\n  BI-RADS 4 focus vector: 152 parameters\n  Keys match: True\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"# Task Analogy","metadata":{}},{"cell_type":"code","source":"enhancement_vector_dict = {}\nfor key in task_vector_birads4.vector.keys():\n    if key in task_vector_birads1.vector:\n        enhancement_vector_dict[key] = task_vector_birads4.vector[key] - task_vector_birads1.vector[key]\n\nprint(f\"‚úì Enhancement vector has {len(enhancement_vector_dict)} parameter groups\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:32:51.763497Z","iopub.execute_input":"2025-11-26T08:32:51.764142Z","iopub.status.idle":"2025-11-26T08:32:51.776306Z","shell.execute_reply.started":"2025-11-26T08:32:51.764119Z","shell.execute_reply":"2025-11-26T08:32:51.775550Z"}},"outputs":[{"name":"stdout","text":"‚úì Enhancement vector has 152 parameter groups\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"alpha = 0.25  # Adjust to test different strengths\nprint(f\"‚úì Using alpha = {alpha} for enhancement scaling\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:33:02.622461Z","iopub.execute_input":"2025-11-26T08:33:02.622758Z","iopub.status.idle":"2025-11-26T08:33:02.627111Z","shell.execute_reply.started":"2025-11-26T08:33:02.622737Z","shell.execute_reply":"2025-11-26T08:33:02.626386Z"}},"outputs":[{"name":"stdout","text":"‚úì Using alpha = 0.25 for enhancement scaling\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# Load base (pretrained) weights and BIRADS 1-5 task vector\nbase_sd = torch.load('clip_visual_pretrained.pt', weights_only=False)\nbirads_1to5_sd = torch.load('clip_visual_imbalanced.pt', weights_only=False)\n\n# Add: base + (birads1-5 vector) + (alpha * enhancement vector)\nfinal_sd = {}\nfor key in base_sd.keys():\n    # Only update layers that exist in all vectors\n    if key in birads_1to5_sd and key in enhancement_vector_dict:\n        final_sd[key] = (\n            base_sd[key]\n            + (birads_1to5_sd[key] - base_sd[key])\n            + alpha * enhancement_vector_dict[key]\n        )\n    elif key in birads_1to5_sd:\n        # No enhancement for this layer, just use BIRADS 1-5 delta\n        final_sd[key] = birads_1to5_sd[key]\n    else:\n        # No delta, use pretrained\n        final_sd[key] = base_sd[key]\n\nprint(f\"‚úì Final synthesized weights created for model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:33:34.937770Z","iopub.execute_input":"2025-11-26T08:33:34.938349Z","iopub.status.idle":"2025-11-26T08:33:35.342679Z","shell.execute_reply.started":"2025-11-26T08:33:34.938325Z","shell.execute_reply":"2025-11-26T08:33:35.341993Z"}},"outputs":[{"name":"stdout","text":"‚úì Final synthesized weights created for model\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# Load weights into a CLIP model\nmodel_clip, _ = clip.load(CONFIG['clip_model'], device=device)\nmodel_clip = model_clip.float()\nmodel_clip.visual.load_state_dict(final_sd, strict=False)\nprint(\"‚úì Synthesized model loaded into CLIP visual encoder\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:33:58.880292Z","iopub.execute_input":"2025-11-26T08:33:58.880567Z","iopub.status.idle":"2025-11-26T08:34:02.996717Z","shell.execute_reply.started":"2025-11-26T08:33:58.880546Z","shell.execute_reply":"2025-11-26T08:34:02.996068Z"}},"outputs":[{"name":"stdout","text":"‚úì Synthesized model loaded into CLIP visual encoder\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"#¬†Run test","metadata":{}},{"cell_type":"code","source":"# Load base (pretrained)\nmodel_clip_base, _ = clip.load(CONFIG['clip_model'], device=device)\nmodel_clip_base = model_clip_base.float()\n\n# Imbalanced visual model\nvisual_imbalanced_sd = torch.load('clip_visual_imbalanced.pt', weights_only=False)\nmodel_clip_imbalanced, _ = clip.load(CONFIG['clip_model'], device=device)\nmodel_clip_imbalanced = model_clip_imbalanced.float()\nmodel_clip_imbalanced.visual.load_state_dict(visual_imbalanced_sd, strict=False)\n\n# Synthesized model: base + imbalanced + alpha*enhancement\nsynthesized_sd = final_sd  # from your previous synthesis cell!\nmodel_clip_synth, _ = clip.load(CONFIG['clip_model'], device=device)\nmodel_clip_synth = model_clip_synth.float()\nmodel_clip_synth.visual.load_state_dict(synthesized_sd, strict=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:36:46.970457Z","iopub.execute_input":"2025-11-26T08:36:46.971115Z","iopub.status.idle":"2025-11-26T08:36:59.169413Z","shell.execute_reply.started":"2025-11-26T08:36:46.971091Z","shell.execute_reply":"2025-11-26T08:36:59.168696Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ndef run_inference(model_clip, val_loader, text_prompts, device):\n    # Encode text prompts once\n    model_clip.eval()\n    with torch.no_grad():\n        text_tokens = clip.tokenize(text_prompts).to(device)\n        text_features = model_clip.encode_text(text_tokens)\n        text_features = text_features / (text_features.norm(dim=-1, keepdim=True) + 1e-8)\n\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for images, texts, labels in val_loader:\n            images = images.to(device).float()\n            labels = labels.to(device)\n            image_features = model_clip.encode_image(images)\n            image_features = image_features / (image_features.norm(dim=-1, keepdim=True) + 1e-8)\n            logit_scale = model_clip.logit_scale.exp().clamp(max=100)\n            logits = logit_scale * image_features @ text_features.T\n            preds = logits.argmax(dim=1)\n\n            all_preds.extend(preds.cpu().tolist())\n            all_labels.extend(labels.cpu().tolist())\n    return all_preds, all_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:37:06.292118Z","iopub.execute_input":"2025-11-26T08:37:06.292370Z","iopub.status.idle":"2025-11-26T08:37:06.298946Z","shell.execute_reply.started":"2025-11-26T08:37:06.292352Z","shell.execute_reply":"2025-11-26T08:37:06.298120Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# Set up prompts for 4-class BI-RADS\nbirads_prompts = [\n    \"a mammogram showing BI-RADS category 1, normal breast tissue\",\n    \"a mammogram showing BI-RADS category 2, benign findings\",\n    \"a mammogram showing BI-RADS category 3, probably benign findings\",\n    \"a mammogram showing BI-RADS category 4, suspicious abnormality\"\n]\n\n# Imbalanced visual model\nimbal_preds, imbal_labels = run_inference(model_clip_imbalanced, val_loader_5class, birads_prompts, device)\nprint(\"\\nImbalanced Visual Model:\")\nprint(classification_report(imbal_labels, imbal_preds, target_names=[\"BI-RADS 1\", \"BI-RADS 2\", \"BI-RADS 3\", \"BI-RADS 4\"], zero_division=0))\n\n# Synthesized model\nsynth_preds, synth_labels = run_inference(model_clip_synth, val_loader_5class, birads_prompts, device)\nprint(\"\\nSynthesized Model (Enhanced):\")\nprint(classification_report(synth_labels, synth_preds, target_names=[\"BI-RADS 1\", \"BI-RADS 2\", \"BI-RADS 3\", \"BI-RADS 4\"], zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:37:37.532619Z","iopub.execute_input":"2025-11-26T08:37:37.533215Z","iopub.status.idle":"2025-11-26T08:37:49.486684Z","shell.execute_reply.started":"2025-11-26T08:37:37.533188Z","shell.execute_reply":"2025-11-26T08:37:49.485902Z"}},"outputs":[{"name":"stdout","text":"\nImbalanced Visual Model:\n              precision    recall  f1-score   support\n\n   BI-RADS 1       0.47      0.80      0.59        60\n   BI-RADS 2       0.00      0.00      0.00        60\n   BI-RADS 3       0.59      0.65      0.62        60\n   BI-RADS 4       0.07      0.17      0.10         6\n\n    accuracy                           0.47       186\n   macro avg       0.28      0.40      0.33       186\nweighted avg       0.34      0.47      0.39       186\n\n\nSynthesized Model (Enhanced):\n              precision    recall  f1-score   support\n\n   BI-RADS 1       0.45      0.75      0.56        60\n   BI-RADS 2       0.00      0.00      0.00        60\n   BI-RADS 3       0.57      0.65      0.61        60\n   BI-RADS 4       0.06      0.17      0.08         6\n\n    accuracy                           0.46       186\n   macro avg       0.27      0.39      0.31       186\nweighted avg       0.33      0.46      0.38       186\n\n","output_type":"stream"}],"execution_count":53}]}